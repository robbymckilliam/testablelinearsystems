
%\todo{Introduce functions abstractly.}

It is assumed that the reader is familiar with the concept of a function!  That is, a map from the elements in a set $X$ to the elements in another set $Y$.  Consider sets
\[
X = \left\{
\begin{array}{c}
\text{Mario} \\
\text{Link} \\
\text{Ness} 
\end{array} 
\right\}
 \qquad 
Y = \left\{
\begin{array}{c}
\text{Freeman} \\
\text{Ryu} \\
\text{Sephiroth} \\
\text{Conker} \\
\text{Ness}
\end{array} 
\right\}.
\]
An example of a function from $X$ to $Y$ is
\[
f(x) = \begin{cases}
\text{Conker} & x = \text{Mario} \\
\text{Sephiroth} & x = \text{Link} \\
\text{Sephiroth} & x = \text{Ness}.
\end{cases}
\]
The function $f$ maps Mario to Conker, Link to Sephiroth, and Ness to Sephiroth.  The value of $f$ for input $x$ is denoted by $f(x)$ and so, for example, $f(\text{Mario}) = \text{Conker}$ and $f(\text{Link}) = \text{Sephiroth}$.  The set $X$ is called the \term{domain} of the function $f$ and the set of values that the functions takes, that is, the set $\{f(x), x \in X\}$, is called the \term{range}.  In the above example the range is the set $\{\text{Conker}, \text{Sephiroth}\}$.  Observe that the range is a subset of $Y$.  The set of all functions mapping $X$ to $Y$ is denoted by $X \to Y$ and so $f \in X \to Y$ in the example above.

A \term{signal} is a function that maps a real number to a complex number, that is, a function from the set $\reals \to \complex$.  For example
\[
\sin( \pi t), \qquad \frac{1}{2} t^3, \qquad e^{-t^2}
\]   
all represent signals with $t \in \reals$.  The real part of these signals is plotted in Figure~\ref{fig:signalsstart}.  Many physical phenomena, such as sound, light, weather, and motion, can be modelled using signals.  In this text we primarily focus on examples from electrical and mechanical engineering where signals are use to model changes in quantities such as voltage, current, position, angle, force, and torque, over time.  In these examples, the independent variable $t$ represents ``time''.  However, there is no fundamental reason for this and the techniques developed here can be applied equally well when $t$ represents a quantity other than time.  An example where this occurs is image processing.

% If $x$ is a signal and $t$ an input variable we write $x(t)$ for the output variable corresponding with $t$.  Signals can be multidimensional.  This page is an example of a 2-dimensional signal, the independent variables are the horizontal and vertical position on the page, and the signal maps this position to a colour, in this case either black or white.  A moving image such as seen on your television or computer monitor is an example of a 3-dimensional signal, the three independent variables being vertical and horizontal screen position and time.  The signal maps each position and time to a colour on the screen.  %Your brain processes an extremely high dimensional signal (many billions of dimensional), each independent variable represented by a electrical or chemical signal in a sensory nerve, such as in your eye, your tongue, your ears and your skin.  The output variable in this case is also in a very large number of dimensions, represented by 
% In these notes we focus exclusively on 1-dimensional signals such as those in Figure~\ref{fig:signalsstart} and we will only consider signals where the output variable is real or complex valued.  Many of the results presented here can be extended to deal with multidimensional signals.

%A 1-dimensional signal is said to be a \term{continuous-time signal} if the variable $t \in \reals$, wheras it is said to be a discrete-time signal
% If the domain of a signal is the set of real numbers $\reals$ the signal is said to be \term{continuous-time}.  If the domain is the set of integers $\ints$, the signal is said to be \term{discrete-time}.  We will usually indicate a discrete-time signal by using $n$ instead of $t$ for the independent variable, so $\sin(\pi n)$ will usually be a discrete-time signal with $n \in \ints$, and $\sin(\pi t)$ will usually be a continuous-time signal with $t \in \reals$.  Examples of discrete time signals are
% \[
% \sin(\tfrac{\pi}{4} n), \qquad e^{-n^2/4}, \qquad n \in \ints
% \]
% and they are plotted using vertical lines and dots as in Figure~\ref{fig:signalsstartdiscrete}.  This course is mostly concerned with continuous-time signals.  After this section, we will not return to discrete-time signals until Section~\ref{sec:sampl-interp}.

\begin{figure}[tp]
\centering
\begin{tikzpicture}[domain=-1.4:1.4,samples=200]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-1.75,0) -- (1.75,0) node[above] {$t$};
    \draw[->] (0,-1.75) -- (0,1.75) node[left] {$\sin( \pi t)$};
    %\draw[color=black] plot[id=x] function{1/x^2} 
    %    node[right] {$f(t) = t^{-2}$};
    \draw[smooth,color=black,thick] plot function{sin(3.14159265359*x)};
    \htick{1} node[pos=0.5,left] {$1$};
    \htick{-1} node[pos=0.5,right] {$-1$};
    % \draw[color=black] plot[id=exp] function{0.05*exp(x)} 
    %    node[right] {$f(t) = \frac{1}{20} e^t$};
\end{tikzpicture} 
\;\;
\begin{tikzpicture}[domain=-1.4:1.4]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-1.75,0) -- (1.75,0) node[above] {$t$};
    \draw[->] (0,-1.75) -- (0,1.75) node[left] {$\frac{1}{2}  t^3$};
    \draw[smooth,color=black,thick] plot function{x*x*x/2};
    \htick{1} node[pos=0.5,right] {$1$};
    \htick{-1} node[pos=0.5,right] {$-1$};
    %\draw[color=black] plot[id=exp] function{0.05*exp(x)} 
    %    node[right] {$f(t) = \frac{1}{20} e^t$};
\end{tikzpicture} 
\;\;
\begin{tikzpicture}[domain=-1.4:1.4]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-1.75,0) -- (1.75,0) node[above] {$t$};
    \draw[->] (0,-1.75) -- (0,1.75) node[left] {$e^{-t^2}$};
    \draw[smooth,color=black,thick] plot function{exp(-x*x)};
    \htick{1} node[pos=0.5,above right] {$1$};
    \htick{-1} node[pos=0.5,right] {$-1$};
\end{tikzpicture}
\caption{Plots of three signals.} \label{fig:signalsstart}
\end{figure}

% \begin{figure}[tp]
% \centering
% \begin{tikzpicture}[domain=-2:2]
%     %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
%     \draw[->] (-2.5,0) -- (2.5,0) node[above] {$n$};
%     \draw[->] (0,-1.75) -- (0,1.75) node[left] {$\sin\big(\tfrac{\pi}{4} n\big)$};
%     %\draw[color=black] plot[id=x] function{1/x^2} 
%     %    node[right] {$f(t) = t^{-2}$};
%     \draw[color=black,ycomb,mark=*,samples=9] plot function{sin(3.14159265359*x/2)};
%     % \draw[color=black] plot[id=exp] function{0.05*exp(x)} 
%     %    node[right] {$f(t) = \frac{1}{20} e^t$};
% \end{tikzpicture} \;\;
% \begin{tikzpicture}[domain=-2:2]
%     %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
%     \draw[->] (-2.5,0) -- (2.5,0) node[above] {$n$};
%     \draw[->] (0,-1.75) -- (0,1.75) node[left] {$e^{-n^2/4}$};
%     \draw[color=black,ycomb,mark=*,samples=9] plot function{exp(-x*x)};
% \end{tikzpicture}
% \caption{1-dimensional discrete-time signals} \label{fig:signalsstartdiscrete}
% \end{figure}

\section{Properties of signals}\label{sec:properties-signals}

%We now list a number of important properties

A signal $x$ is \term{bounded} if there exists a real number $M$ such that 
\[
\sabs{x(t)} < M \qquad \text{for all $t \in \reals$} 
\]
where $\sabs{\cdot}$ denotes the (complex) magnitude.  Both $\sin( \pi t)$ and $e^{-t^2}$ are examples of bounded signals because $\sabs{\sin( \pi t)} \leq 1$ and $\sabs{e^{-t^2}} \leq 1$ for all $t \in \reals$.  However, $\frac{1}{2}t^3$ is not bounded because its magnitude grows indefinitely as $t$ moves away from the origin.  % Similarly a discrete-time signal $x$ is bounded if there exists a real number $M$ such that 
% \[
% \sabs{x(n)} \leq M \qquad \text{for all $n \in \ints$}.
% \]
% For example, both $\sin(\tfrac{\pi}{4} n)$ and $e^{-n^2/4}$ are bounded, but the signal $x(n) = n$ is not.

A signal $x$ is \term{periodic} if there exists a positive real number $T$ such that
\[
x(t) = x(t + kT) \qquad \text{for all $k \in \ints$ and $t \in \reals$.}
\]
The smallest such positive $T$ it is called the \term{fundamental period} or simply the~\term{period}.  For example, the signal $\sin( \pi t)$ is periodic with period $T=2$.  Neither $\frac{1}{2}t^3$ or $e^{-t^2}$ are periodic.  % A discrete-time signal $x(n)$ is periodic if there exists an integer $T$ such that 
% \[
% x(n) = x(n + kT) \qquad \text{for all $k \in \ints$ and $n \in \ints$}.
% \]
% For example $\sin(\tfrac{\pi}{4} n)$ is periodic but $e^{-n^2/4}$ is not.

A signal $x$ is \term{right sided} if there exists $T \in \reals$ such that $x(t) = 0$ for all $t < T$.  Correspondingly $x$ is \term{left sided} if $x(t) = 0$ for all $T > t$.  For example, the \term{step function} 
\begin{equation} \label{eq:stepfunction}
u(t) = \begin{cases}
1 & t \geq 0 \\
0 & t < 0
\end{cases}
\end{equation}
is right-sided.  Its horizontal reflection $u(-t)$ is left sided (Figure~\ref{fig:stepsided}).  A signal $x$ is said to be \term{finite} or to have \term{finite support} if it is both left and right sided, that is, if there exists $T\in\reals$ such that $x(t) = x(-t) = 0$ for all $t > T$.  The signals $\sin( \pi t)$ and $e^{-t^2}$ do not have finite support, but the \term{rectangular pulse}
\begin{equation}\label{eq:rectfuncdefn}
\rect(t) = \begin{cases} 
1 & \abs{t} < \frac{1}{2}\\
0 & \text{otherwise}
\end{cases}
\end{equation}
does.

\begin{figure}[tp]
\centering
\begin{tikzpicture}[domain=-2:2]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-2.5,0) -- (2.5,0) node[above] {$t$};
    \draw[->] (0,-0.65) -- (0,1.75) node[left] {$u(t)$};
    \draw[thick] (-2,0) -- (0,0) -- (0,1) -- (2,1);
    %\draw[color=black] plot[id=exp] function{0.05*exp(x)} 
    %    node[right] {$f(t) = \frac{1}{20} e^t$};
\end{tikzpicture} \;\;
\begin{tikzpicture}[domain=-2:2]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-2.5,0) -- (2.5,0) node[above] {$t$};
    \draw[->] (0,-0.65) -- (0,1.5) node[left] {$u(-t)$};
    \draw[thick] (2,0) -- (0,0) -- (0,1) -- (-2,1);
\end{tikzpicture}
\begin{tikzpicture}[domain=-2:2]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-2.5,0) -- (2.5,0) node[above] {$t$};
    \draw[->] (0,-0.65) -- (0,1.75) node[left] {$\Pi(t)$};
    \draw[thick] (-2,0) -- (-0.5,0) -- (-0.5,1) -- (0.5,1) -- (0.5,0) -- (2,0);
    \vtick{-0.5} node[pos=0.5,below] {$-\tfrac{1}{2}$};
    \vtick{0.5} node[pos=0.5,below] {$\tfrac{1}{2}$};
    %\draw[color=black] plot[id=exp] function{0.05*exp(x)} 
    %    node[right] {$f(t) = \frac{1}{20} e^t$};
\end{tikzpicture} \;\;
\begin{tikzpicture}[domain=-2:2]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-2.5,0) -- (2.5,0) node[above] {$t$};
    \draw[->] (0,-0.65) -- (0,1.5) node[left] {$\tfrac{1}{2} + \tfrac{1}{2}\cos(\pi t)$};
    \draw[smooth,color=black,thick] plot function{0.5 + 0.5*cos(3.14159265359*x)};
        %\vtick{-1} node[pos=0.5,below] {$-1$};
    %\vtick{1} node[pos=0.5,below] {$1$};
\end{tikzpicture}

%\\
% \begin{tikzpicture}
%     %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
%    \draw[->] (-2.5,0) -- (2.5,0) node[above] {$n$};
%     \draw[->] (0,-0.5) -- (0,1.5) node[left] {$u(n)$};
%      \foreach \n in {-2,-1.5,-1,-0.5,0}{
%       \draw plot[ycomb,mark=*] (\n,0);
%     }
%     \foreach \n in {0.5,1,1.5,2}{
%       \draw plot[ycomb,mark=*] (\n,1);
%     }
% \end{tikzpicture} \;\;
% \begin{tikzpicture}
%     %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
%    \draw[->] (-2.5,0) -- (2.5,0) node[above] {$n$};
%     \draw[->] (0,-0.5) -- (0,1.5) node[left] {$u(-n)$};
%     \foreach \n in {-2,-1.5,-1,-0.5}{
%       \draw plot[ycomb,mark=*] (\n,1);
%     }
%     \foreach \n in {0,0.5,1,1.5,2}{
%       \draw plot[ycomb,mark=*] (\n,0);
%     }
% \end{tikzpicture}
\caption{The right sided step function $u(t)$, its left sided reflection $u(-t)$, the finite rectangular pulse $\Pi(t)$ and the signal $\tfrac{1}{2} + \tfrac{1}{2}\cos(x)$ that is not finite.  %The definition of left and right sided applies to both continuous and discrete-time signals.
} 
\label{fig:stepsided}
\end{figure}

A signal $x$ is \term{even} (or \term{symmetric}) if 
\[
x(t) = x(-t) \qquad \text{for all $t \in \reals$}
\] 
and \term{odd} (or \term{antisymmetric}) if 
\[
x(t) = -x(-t) \qquad \text{for all $t \in \reals$}.
\]  
For example, $\sin(\pi t)$ and $\tfrac{1}{2}t^3$ are odd and $e^{-t^2}$ is even.  % Every signal $x$ can be expressed as a sum $x = x_e + x_o$ where 
% \[
% x_e(t) = \tfrac{1}{2}\big( x(t) + x(-t) \big)
% \]
% is an even signal called the \term{even part} of $x$ and 
% \[
% x_o(t) = \tfrac{1}{2}\big( x(t) - x(-t) \big)
% \] 
% is an odd signal called the \term{odd part} of $x$.  For example, put $x(t) = e^{-t^2} + \sin(\pi t)$.  The even part of $x$ is $x_e(t) = e^{-t^2}$ and the odd part is $x_o(t) = \sin(\pi t)$.
A signal $x$ is \term{conjugate symmetric} if 
\[
x(t) = x(-t)^* \qquad \text{for all $t \in \reals$}
\]
and \term{conjugate antisymmetric} if 
\[
x(t) = -x(-t)^* \qquad \text{for all $t \in \reals$},
\]
where $*$ denotes the complex conjugate of a complex number.  Equivalently, $x$ is conjugate symmetric if its real part $\Re x$ is an even signal and its imaginary part $\Im x$ is an odd signal, and $x$ is conjugate antisymmetric if its real part is odd and its imaginary part is even.  For example, the signal $e^{-t^2} + j \sin(\pi t)$ where $j = \sqrt{-1}$ is conjugate symmetric and the signal $\tfrac{1}{2}t^{3} + j e^{-t^2}$ is conjugate antisymmetric.

A signal $x$ is \term{continuous at} $t \in \reals$ if
\[
\lim_{h \to 0} x(t + h) = \lim_{h \to 0} x(t-h)
\]
and $x$ is said to be \term{continuous} if it is continuous at all $t \in \reals$.  The signals $\sin( \pi t)$, $\frac{1}{2} t^3$, and $e^{-t^2}$ are continuous, but the step function $u$ is not continuous at zero because
\[
\lim_{h \to 0} u(h) = 1 \neq 0 = \lim_{h \to 0} u(-h).
\]
The set of continuous signals is typically denoted by $C^0(\reals)$ or just $C^0$.  A signal $x$ is \term{continuously differentiable} or just \term{differentiable} if
\[
\lim_{h \to 0} \frac{x(t + h) - x(t)}{h} = \lim_{h \to 0} \frac{x(t) - x(t-h)}{h} \qquad \text{for all $t \in \reals$}.
\]
Considered as a function of $t$ this limit is called the \term{derivative} of $x$ at $t$ and is typically denoted by $\frac{d}{dt} x(t)$.  For example, the signals $\sin( \pi t)$, $\frac{1}{2} t^3$, $e^{-t^2}$, and $t^2$ are differentiable with derivatives
\[
\pi \cos(\pi t), \qquad \tfrac{3}{2} t^{2}, \qquad -2t e^{-t^2}, \qquad 2t,
\] 
but the step function $u$ and the rectangular pulse $\rect$ are not differentiable (Exercise~\ref{exer:steprectnotdiff}).  The set of differentiable signals is denoted by $C^1$ or $C^1(\reals)$.  A signal is \term{$k$-times differentiable} if its $k-1$ th derivative is differentiable.  The set of $k$-times differentiable signals is typically denoted by $C^k$ or $C^k(\reals)$.


\begin{figure}[tp]
\centering
\begin{tikzpicture}[domain=-1.4:1.4,samples=200]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-1.75,0) -- (1.75,0) node[above] {$t$};
    \draw[->] (0,-1.75) -- (0,1.75) node[left] {$\pi\cos( \pi t)$};
    %\draw[color=black] plot[id=x] function{1/x^2} 
    %    node[right] {$f(t) = t^{-2}$};
    \begin{scope}[yscale=0.45]
      \draw[smooth,color=black,thick] plot function{pi*cos(pi*x)};
      \htick{pi} node[pos=0.5,right] {$\pi$};
      \htick{-pi} node[pos=0.5,right] {$-\pi$};
    \end{scope}
      % \draw[color=black] plot[id=exp] function{0.05*exp(x)} 
    %    node[right] {$f(t) = \frac{1}{20} e^t$};
\end{tikzpicture} 
\;\;
\begin{tikzpicture}[domain=-1.4:1.4]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-1.75,0) -- (1.75,0) node[above] {$t$};
    \draw[->] (0,-1.75) -- (0,1.75) node[left] {$\frac{3}{2}  t^2$};
    \begin{scope}[yscale=0.45]
      \draw[smooth,color=black,thick] plot function{3*x*x/2};
      \htick{3} node[pos=0.5,right] {$3$};
      \htick{-3} node[pos=0.5,right] {$-3$};
    \end{scope}
    %\draw[color=black] plot[id=exp] function{0.05*exp(x)} 
    %    node[right] {$f(t) = \frac{1}{20} e^t$};
\end{tikzpicture} 
\;\;
\begin{tikzpicture}[domain=-1.4:1.4]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-1.75,0) -- (1.75,0) node[above] {$t$};
    \draw[->] (0,-1.75) -- (0,1.75) node[left] {$-2 t e^{-t^2}$};
    \htick{1} node[pos=0.5,right] {$1$};
    \htick{-1} node[pos=0.5,right] {$-1$};
    \draw[smooth,color=black,thick] plot function{-2*x*exp(-x*x)};
\end{tikzpicture}
\caption{Derivatives of the signals $\sin( \pi t)$, $\frac{1}{2} t^3$, $e^{-t^2}$  from Figure~\ref{fig:signalsstart}.} \label{fig:signalsstartdiff}
\end{figure}


A signal $x$ is \term{locally integrable} if
\[
\int_{a}^{b} \abs{x(t)} dt < \infty
\]
for all finite constants $a$ and $b$, where $< \infty$ means that the integral evaluates finite complex number.  The signals $\sin(\pi t)$, $\tfrac{1}{2}t^3$, and $e^{-t^2}$ are all locally integrable.  An example of a signal that is not locally integrable is $x(t) = \frac{1}{t}$ (Exercise~\ref{exer:oneontnotlocallyint}).  The set of locally integrable signals is typically denoted by $L_{\text{loc}}$ or $L_{\text{loc}}(\reals)$.

%In this course we always assume that signals are locally integrable, that is, signals are locally integrable functions mapping $\reals \to \reals$ or $\reals \to \complex$.  
% Similarly two discrete time signals $x$ and $y$ are equal if $x(n) = y(n)$ for all $n \in \ints$.
%BLERG: Potentially equality of signals could be defined using local integrability, i.e. two signals are equal if they are equal everywhere except a set of measure zero.
%BERLG: Another relevant term is ``locally bounded'', i.e, a function is locally bounded if for each finite $a$ and $b$ there exists an $M$ such that $\abs{f(x) }< M$ for all $.  ACTUALLY THIS WONT BE GOOD, SINCE THEN FOURIER TRANSFORMS WONT BE SIGNALS.  It's still a useful concept potentially.  
A signal $x$ is \term{absolutely integrable} or \term{Lebesgue integrable} if
\begin{equation}\label{eq:l1normdefncontinuous}
\|x\|_1 = \int_{-\infty}^{\infty} \abs{x(t)} dt < \infty.
\end{equation}
Here we introduce the notation $\|x\|_1$ called the \term{$L^1$-norm} of $x$.  For example $\sin( \pi t)$ and $\tfrac{1}{2}t^3$ are not absolutely integrable, but $e^{-t^2}$ is because~\citep{nicholas_1950_erf}
\begin{equation}\label{eq:expabssum}
\int_{-\infty}^{\infty} \sabs{e^{-t^2}} dt = \int_{-\infty}^{\infty} e^{-t^2} dt  = \sqrt{\pi}.
\end{equation}
% A discrete-time signal $x$ is called \term{absolutely summable} if
% \begin{equation}\label{eq:l1normdiscrete}
% \|x\|_1 = \sum_{n\in\ints} \abs{x(n)}
% \end{equation}
% exists.  For example $\sin(\tfrac{1}{4}n)$ is not absolutely summable, but $e^{-n^2/4}$ is (Excersize~\ref{excer:disctimeabssummableexpbound}).  We have reused (overloaded) the notation $\|\cdot\|_1$ here so that is applies to both continuous and discrete-time signals.  It is left to the reader to remember that $\|\cdot\|_1$ applied to a continuous-time signal means the integral~\eqref{eq:l1normdefncontinuous}, but when applied to a discrete-time signal means the sum~\eqref{eq:l1normdiscrete}.
It is common to denote the set of absolutely integrable signals by $L^1$ or $L^1(\reals)$.  So, $e^{-t^2} \in L^1$ and $\tfrac{1}{2}t^3 \notin L^1$.   A signal $x$ is \term{square integrable} if
\[
\|x\|_2^2 = \int_{-\infty}^{\infty} \abs{x(t)}^2 dt < \infty.
\]
The real number $\|x\|_2$ is called the \term{$L^2$-norm} of $x$.  Square integrable signals are also called \term{energy signals} and the squared $L^2$-norm $\|x\|_2^2$ is called the \term{energy} of $x$. For example, $\sin( \pi t)$ and $\tfrac{1}{2}t^3$ are not energy signals, but $e^{-t^2}$ is.  It has energy $\|e^{-t^2}\|_2^2 = \sqrt{\pi/2}$ (Exercise~\ref{excer:energyexpchangevar}). The set of square integrable signals is denoted by $L^2$ or $L^2(\reals)$.  %A discrete-time signal $x$ is an \term{energy signal} if it is \term{square summable}, that is, if
% \[
% \|x\|_2 = \sum_{n\in\ints} \abs{x(n)}^2
% \]
% exists.  In this case, the $\|x\|_2$ the energy of $x$.  For example, $\sin(\tfrac{\pi}{4} n)$ is not an energy signal, but $e^{-n^2/4}$ is (Excersize~\ref{excer:disctimeeneryexpbound}).  We have again overloaded the notation so that $\|\cdot\|_2$ applies to both continuous and discrete-time signals.

We write $x = y$ to indicate that two signals $x$ and $y$ are~\term{equal pointwise}, that is, $x(t) = y(t)$ for all $t \in \reals$.  This definition of equality is often stronger than we desire.  For example, the step function $u$ and the signal
\[
z(t) = \begin{cases}
1 & t > 0 \\
0 & t \leq 0
\end{cases}
\]
are not equal pointwise because they are not equal at $t=0$ since $u(0) = 1$ and $z(0) = 0$. %although $\rect(t)$ does equal $\rect(-t)$ for all other values of $t \in \reals$.  
It is useful to identify signals that differ only at isolated points and for this we use a weaker definition of equality.  We say that two signals $x$ and $y$ are equal \term{almost everywhere} if
\[
\int_{a}^b \abs{ x(t) - y(t) } dt = 0
\]
for all finite constants $a$ and $b$.  So, in the previous example, while $u \neq z$ pointwise we do have $u = z$ almost everywhere.  Typically the term almost everywhere is abbreviated to a.e. and one writes 
\[
x = y \;\; \text{a.e.} \qquad \text{or} \qquad x(t) = y(t) \;\; \text{a.e.}
\] 
to indicate that the signals $x$ and $y$ are equal almost everywhere.  %This convention is not followed in these notes.  Instead equal almost everywhere will be used by default, that is, by $x = y$ we will always mean that $x = y$ almost everywhere.  If it is ever important that two signals are equal pointwise this will be specifically stated.

% A continuous-time signal is called a \term{power signal} if it has finite average energy per unit time.  That is, if
% \[
% \lim_{T \rightarrow \infty} \frac{1}{2T} \int_{-T}^{T} \abs{x(t)}^2 dt
% \]
% exists.  If the limit above exists its value is called the \term{power} of the signal $x(t)$.  An energy signal is a power signal with power equal to zero.  The signal $\sin( \pi t)$ is a power signal with power
% \[
% \lim_{T \rightarrow \infty} \frac{1}{2T} \int_{-T}^{T} \sin^2( \pi t) dt = \tfrac{1}{2}.
% \] 
% The signal $e^{-t^2}$ is an energy signal, so it is a power signal with zero power.  The signal $\tfrac{1}{2}t^3$ is not a power signal because
% \[
% \lim_{T \rightarrow \infty} \frac{1}{2T} \int_{-T}^{T} \tfrac{1}{4} t^6 dt = \lim_{T \rightarrow \infty} \frac{T^6}{56}
% \]
% and this limit does not exists (it diverges).

% A discrete-time signal is a \term{power signal} if its average energy is finite, that is,
% \[
% \lim_{T\rightarrow\infty} \frac{1}{2T+1} \sum_{n=-T}^{T} \abs{x(n)}^2
% \]
% exists.  If the limit exists its value is called the power of $x(n)$.  For example, $\sin(\tfrac{\pi}{4} n)$ is a power signal with power $\tfrac{1}{2}$, and $e^{-n^2/4}$ is an energy signal, so it is also a power signal with zero power.  However, the discrete-time signal $x(n)=n$ is not a power signal because
% \[
% \lim_{T\rightarrow\infty} \frac{1}{2T+1} \sum_{n=-T}^{T} n^2 = \lim_{T\rightarrow\infty} \tfrac{1}{3}T(T+1)
% \]
% and this limit diverges.


%BLERG: It's potentially a decent idea to make signals have contain there limits from the left, i.e. be continuous from the left.  This works well if you define the differentiator, integrator etc in the right way.  Makes sense of impulse response etc. 


\section{Spaces of signals}\label{sec:spaces-signals}

%The domain and range of a system are subsets of the set of signals $\reals \to \complex$. Before further describing the properties of systems 
We will regularly be interested in subsets of the set of all signals $\reals \to \complex$.  Two important families of subsets are the \term{linear spaces} and the \term{shift-invariant spaces}.

Let $x$ and $y$ be signals.  We denote by $x + y$ the signal that takes the value $x(t) + y(t)$ for each $t  \in \reals$, that is, the signal that results from adding $x$ and $y$.  For $a \in \complex$ we denote by $ax$ the signal that takes the value $a x(t)$ for each $t \in \reals$, that is, the signal that results from multiplying $x$ by $a$ (Figure~\ref{fig:multiandaddofsignals}).  For signals $x$ and $y$ and complex numbers $a$ and $b$ the signal
\[
ax + by
\]
is called a \term{linear combination} of $x$ and $y$.  %Figure~\ref{fig:multiandaddofsignals} plots examples of 

Let $X \subseteq \reals \to \complex$ be a set of signals.  The set $X$ is a \term{linear space} (or \term{vector space}) if for all signals $x$ and $y$ from $X$ and all complex numbers $a$ and $b$ the linear combination $ax + by$ is also in $X$.  The set of all signals $\reals \to \complex$ is a linear space.  Another example is the set of differentiable signals, because, if $x$ and $y$ are differentiable, then the linear combination $ax + by$ is differentiable.  The derivative is $a D x + b D y$.  The set of even signals is another example of a linear space because if $x$ and $y$ are even then 
\[
ax(t) + by(t) = a x(-t) + b y(-t)
\]
and so the linear combination $ax + by$ is even.  The set of locally integrable signals $L_{\text{loc}}$, the set of absolutely integrable signals $L^1$, and the set of square integrable signals $L^2$ are linear spaces (Exercise~\ref{exer:L1L2linearshiftinvariant}).  The set of periodic signals is not a linear space (Exercise~\ref{exer:periodicshiftinvariantnotlinear}).

For a real number $\tau$ the signal $x(t - \tau)$ is called a \term{time-shift} or \term{shift} or sometimes \term{translation} of the signal $x(t)$.  Figure~\ref{fig:timeshifter} depicts the shift $x(t - \tau)$ for different values of $\tau$ in the case that $x(t) = e^{-t^2}$.   A set of signals $X \subseteq \reals \to \complex$ is a \term{shift-invariant space} if for all $x \in X$ and all $\tau \in \reals$ the shift $x(t - \tau)$ is also in $X$.  Examples of shift-invariant spaces are the set of differentiable signals, the set of periodic signals (Exercise~\ref{exer:periodicshiftinvariantnotlinear}), and $L_{\text{loc}}$, $L^1$, and $L^2$~\eqref{exer:L1L2linearshiftinvariant}.  The set of even signals and the set of odd signals are not shift-invariant spaces.

Linear spaces and shift-invariant spaces of signals will act as domains for the next type of function that we consider called \term{systems}.

\begin{figure}[tp]
\centering
\def\scalex{1.5}
\def\scaley{1.5}
\begin{tikzpicture}[domain=-1.4:1.4,samples=200, yscale=\scaley,xscale=\scalex]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-1.75,0) -- (1.75,0) node[above] {$t$};
    \draw[->] (0,-1.25) -- (0,2.3) node[left] {};
    %\draw[color=black] plot[id=x] function{1/x^2} 
    %    node[right] {$f(t) = t^{-2}$};
      \draw[smooth,color=black,thick] plot function{exp(-x*x)};
      \draw[smooth,color=black,thick] plot function{-0.5*exp(-x*x)};
      \draw[smooth,color=black,thick] plot function{2*exp(-x*x)};
      \begin{scope}[xscale=1/\scalex]
%        \htick{1} node[pos=0.5,above right] {$1$};
        %\htick{-1.25} node[pos=0.5,below right] {$-\tfrac{5}{4}$};
      \end{scope}
      \node at (0.5,1.95) {$2x$};
      \node at (-0.3,1.15) {$x$};
      \node at (1,-0.5) {$-\tfrac{1}{2}x$};
      % \draw[color=black] plot[id=exp] function{0.05*exp(x)} 
    %    node[right] {$f(t) = \frac{1}{20} e^t$};
\end{tikzpicture} 
\;\;
\begin{tikzpicture}[domain=-1.4:1.4, xscale=\scalex, yscale=\scaley]
    %\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-1.75,0) -- (1.75,0) node[above] {$t$};
    \draw[->] (0,-1.25) -- (0,2.3) node[left] {};
%    \begin{scope}[yscale=0.45]
      \draw[smooth,color=black,thick] plot function{exp(-x*x)};
      \draw[smooth,color=black,thick] plot function{sin(pi*x)};
      \draw[smooth,color=black,thick] plot function{exp(-x*x) + sin(pi*x)};
      \begin{scope}[xscale=1/\scalex]
%        \htick{1} node[pos=0.5,right] {$1$};
%        \htick{-1} node[pos=0.5,right] {$-1$};
      \end{scope}
      \node at (-1.0,-0.7) {$y$};
      \node at (-0.7,0.8) {$x$};
      \node at (1,1.6) {$x+y$};
%    \end{scope}
    %\draw[color=black] plot[id=exp] function{0.05*exp(x)} 
    %    node[right] {$f(t) = \frac{1}{20} e^t$};
\end{tikzpicture} 
\caption{The signal $x(t) = e^{-t^2}$ and the signals $2x$ and $-\tfrac{1}{2} x$ (left).  The signals $x(t) = e^{-t^2}$ and $y(t) = \sin(\pi t)$ and the signal $x + y$.} \label{fig:multiandaddofsignals}
\end{figure}


\begin{figure}[p]
  \centering
  \begin{tikzpicture}[domain=-5.5:4.5,samples=200]
    % \draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-6,0) -- (5,0) node[above] {$t$};
    \draw[->] (0,-0.3) -- (0,2.5);
    \draw[smooth,color=black,thick] plot function{2*exp(-x*x)};
    \draw[smooth,color=black,thick] plot function{2*exp(-(x+3)*(x+3))};
    \draw[smooth,color=black,thick] plot function{2*exp(-(x-1.5)*(x-1.5))};
    \node[left,shift={(0.0,2.2)}] {$x$};
    \node[shift={(1.5,2.2)}] {$x(t - 1.5)$};
    \vtick{1.5} node[pos=0.5,below] {$1.5$};
    \node[shift={(-3,2.2)}] {$x(t + 3)$};
    \vtick{-3} node[pos=0.5,below] {$-3$};
  \end{tikzpicture}
  \caption{The signal $x(t) = e^{-t^2}$ and shift $x(t - \tau) = e^{-(t-\tau)^2}$ for $\tau = 1.5$ and $-3$.} \label{fig:timeshifter}
\end{figure}

% \todo{Abstract properties of vector space}

% \todo{Examples: even/odd signals, periodic signals, differentiable signal, shift spaces, absolutely integrable and square integrable signals etc}

% The set of all differentiable signals forms a linear space because if $x$ and $y$ are differentiable signals 

% \todo{Normed vector spaces, $L^1$, $L^2$}



\section{Systems (functions of signals)}\label{sec:syst-funct-sign}

A \term{system} is a function that maps a signal to another signal.  For example,
\[
x(t) + 3 x(t-1), \qquad \int_{0}^{1} x(t - \tau) d\tau, \qquad \frac{1}{x(t)}, \qquad \frac{d}{dt} x(t)
\]
represent systems, each mapping the signal $x$ to another signal.  
%More precisely, let $X \subseteq \reals \to \complex$ and $Y \subseteq \reals \to \complex$ be sets of signals.  A system is a function $H \
Consider the electric circuit in Figure~\ref{circ:voltagedivider} called a \term{voltage divider}.  If the voltage at time $t$ is $x(t)$ then, by Ohm's law, the current at time $t$ satisfies
\[
i(t) = \frac{1}{R_1 + R_2} x(t),
\]
and the voltage over the resistor $R_2$ is
\begin{equation}\label{eq:voltagedivider}
y(t) = R_2 i(t) = \frac{R_2}{R_1 + R_2} x(t).
\end{equation}
The circuit can be considered as a system mapping the signal $x$ representing the voltage to the signal $i = \tfrac{1}{R_1+R_2}x$ representing the current, or a system mapping $x$ to the signal $y=\frac{R_2}{R_1 + R_2}x$ representing the voltage over resistor $R_2$.

\begin{figure}[tp]
\centering
\begin{circuitikz} \draw
 %(0,0) node[anchor=east]{B}
  to[short, i<=$i(t)$, o-] (3,0)
  to[R,l=$R_2$] (3,3)
 (3,3) to[R, l=$R_1$, -o] (0,3)
 to[open, v=$x(t)$] (0,0)
 (3,0) to[short,-o] (4,0)
 (3,3) to[short,-o] (4,3)
 (4,0) to[open, v>=$y(t)$] (4,3)
;\end{circuitikz}
\caption{A \term{voltage divider} circuit.} \label{circ:voltagedivider}
\end{figure}

%%some styles for block diagrams
\begin{figure}[tp]
\centering
\begin{tikzpicture}[node distance=2.5cm,auto]
    \node [dspsquare] (H) {$H$};
    \node [dspnodeopen,dsp/label=above] (x) [left of=H,node distance=2cm] {$x$};
    \node [dspnodeopen,dsp/label=above] (y) [right of=H, node distance=2cm] {$Hx$};
    \draw[dspconn] (x) -- (H);
    \draw[dspflow] (H) -- (y);
\end{tikzpicture}
\caption{System block diagram with input signal $x$ and output signal $H(x)$.}\label{fig:blockdiagramH1}
\end{figure}


% \begin{figure}[tp]
% \centering
% \begin{circuitikz} \draw
%  %(0,0) node[anchor=east]{B}
%   to[R,l=$R_o$, i<=$i(t)$, o-] (3,0)
%   to[R,l=$R_2$] (3,3)
%  (3,3) to[R, l=$R_1$, -o] (0,3)
%  to[open, v=$x(t)$] (0,0)
%  (3,0) to[short,-o] (5,0)
%  (3,3) to[short,-o] (5,3)
%  (4,0) to[R,l_=$R_i$] (4,3)
%  (5,0) to[open, v>=$y(t)$] (5,3)
% ;\end{circuitikz}
% \caption{A \term{voltage divider} circuit including input resistance $R_i$ and output resistance $R_o$.} \label{circ:voltagedividerinpout}
% \end{figure}

Let $X \subseteq \reals \to \complex$ and $Y \subseteq \reals \to \complex$ be sets of signals.  We denote systems with capital letters such as $H$ and $G$.  A system is a function $H \in X \to Y$ that maps each signal from the domain $X$ to a signal from $Y$.  Given \term{input signal} $x \in X$ the \term{output signal} of the system is denoted by $H(x)$.  The output signal is often called the \term{response} of system $H$ to signal $x$.  We will often drop the brackets and write simply $Hx$ for the response of $H$ to $x$\footnote{In the literature it is customary to drop the brackets only when $H$ is a \term{linear} system (Section~\ref{sec:properties-systems}).  In this text the we occasionally drop the brackets even when $H$ is not linear.  Since we deal primarily with linear systems this faux pas will occur rarely.}.  The value of the output signal $Hx$ at $t \in \reals$ is denoted by $Hx(t)$ or $H(x)(t)$ or $H(x,t)$ and we do not distinguish between these notations.   %~\citep{Schönfinkel_currying_1924,Currying_1968}.  
It is sometimes useful to depict systems with a block diagram as in Figure~\ref{fig:blockdiagramH1}.  The electric circuit in Figure~\ref{circ:voltagedivider} corresponds with the system
\[
Hx = \frac{R_2}{R_1 + R_2} x = y.
\]
This system multiplies the input signal $x$ by $\frac{R_2}{R_1 + R_2}$.  This brings us to our first practical test.

\begin{test}\label{test:voltagedividertest1}
(\textbf{Voltage divider})
In this test we construct the voltage divider from Figure~\ref{circ:voltagedivider} on a breadboard with resistors $R_1 \approx 100\si{\ohm}$ and $R_2 \approx 470\si{\ohm}$ with values accurate to within 5\%.  Using a computer soundcard (an approximation of) the voltage signal 
\[
x(t) = \sin( 2 \pi f_1 t) \qquad  \text{with} \qquad  f_1 = 100
\] 
is passed through the circuit.  The approximation is generated by sampling $x(t)$ at rate $F = \frac{1}{P} = 44100\si{\hertz}$ to generate samples 
\[
x(n P) \qquad n = 0, \dots, 2 F
\]
corresponding to approximately $2$ seconds of signal.  These samples are passed to the soundcard which starts playback.  The voltage over resistor $R_2$ is recorded (also using the soundcard) that returns a list of samples $y_1,\dots,y_L$ taken at rate $F$.  The voltage over $R_2$ can be (approximately) reconstructed from these samples as
\begin{equation}\label{eq:yreconstruct}
\tilde{y}(t) = \sum_{\ell=1}^L y_\ell \sinc( F t - \ell )
\end{equation}
where
\begin{equation}\label{eq:sincfunction}
\sinc(t) = \frac{\sin(\pi t)}{\pi t}
\end{equation}
is the called the \term{sinc function} and is plotted in Figure~\ref{fig:sincfunction1}. We will justify this reconstruction in Section~\ref{sec:bandlimited-signals}.  Simultaneously the (stereo) soundcard is used to record the input voltage $x$ producing samples $x_1,\dots,x_L$ taken at rate $F$.  An approximation of the input signal is  
\begin{equation}\label{eq:xreconstruct}
\tilde{x}(t) = \sum_{\ell=1}^L x_\ell \sinc( F t - \ell ).
\end{equation}
In view of~\eqref{eq:voltagedivider} we would expect the approximate relationship
\[
\tilde{y} \approx \frac{R_2}{R_1 + R_2} \tilde{x} = \frac{47}{57}\tilde{x}.
\]
A plot of $\tilde{y}$, $\tilde{x}$ and $\tfrac{47}{57}\tilde{x}$ over a $20\si{\milli\second}$ period from $1\si{\second}$ to $1.02\si{\second}$ is given in Figure~\ref{fig:test1voltagedivider}.  The hypothesised output signal $\tfrac{47}{57}\tilde{x}$ does not match the observed output signal $\tilde{y}$.  A primary reason is that the circuitry inside the soundcard itself cannot be ignored.  When deriving the equation for the voltage divider we implicitly assumed that current flows through the output of the soundcard without resistance (a short circuit), and that no current flows through the input device of the soundcard (an open circuit).  These assumptions are not realistic.  Modelling the circuitry in the sound card wont be attempted here.  In Section~\ref{sec:active-circuits} we will construct circuits that contain external sources of power (active circuits).  These are less sensitive to the circuitry inside the soundcard.  

%The reason is that the resistances inside the soundcard itself cannot be ignored.  We can model these resistances using an output resistance $R_o$ for the soundcard output device and an input resistance $R_i$ for the soundcard input device as in Figure~\ref{circ:voltagedividerinpout}.  Appendix~\ref{sec:estim-outp-input} describes a method for estimating $R_o$ and $R_i$.  The hardware used for this test have $R_o \approx 0 \si{\ohm}$ and $R_i \approx 4700\si{\ohm}$.  We now expect the relationship
% \[
% \tilde{y} \approx \frac{R_2^\prime}{R_1^\prime + R_2^\prime} \tilde{x} = \frac{3196}{4139}\tilde{x} \approx 0.77\tilde{x}
% \]
% where 
% \[
% R_1^\prime = R_1 + R_o = 820,\qquad R_2^\prime = \frac{R_2 R_i}{R_2 + R_i} = \frac{63920}{23}. 
% \]
% A plot of $\tilde{y}$, $\tilde{x}$ and $\frac{47}{82}\tilde{x}$ is given in Figure~\ref{fig:test1voltagedividerfixed}.  Observe that $\tilde{y}$ is close to $0.77\tilde{x}$.

% %%some styles for block diagrams
% \begin{figure}[p]
% \centering
% \includegraphics{tests/voltagedivider/plot-2.mps}
% \caption{Plot of reconstructed input signal $\tilde{x}$ (solid line), output signal $\tilde{y}$ (solid line with circle) and hypothesised output signal $0.77\tilde{x}$ (solid line with dot) for the voltage divider circuit including input and output resistances in Figure~\ref{circ:voltagedividerinpout}.} \label{fig:test1voltagedividerfixed}
% \end{figure}

\end{test}


%BLERG Potentially what this next bit is out of place?


When specifying a system we are free to choose the domain $X$ at our convenience.  In cases such as the voltage divider it is reasonable to choose the domain $X = \reals \to \complex$, that is, the domain can contain \emph{all} signals.  However, this is not always convenient or possible.  For example, the system
\[
Hx(t) = \frac{1}{x(t)}
\]
is not defined at those $t$ where $x(t) = 0$ because we cannot divide by zero.  To avoid this we might choose the domain as the set of signals $x(t)$ that are not zero for any $t \in \reals$.

Another example is the system $I_\infty$ defined by
\begin{equation}\label{eq:deifferentiator}
I_{\infty}x(t) = \int_{-\infty}^{t} x(\tau) d\tau ,
\end{equation}
called an \term{integrator}.  %, that is not defined for those signals where the integral above does not exist. %BLERG: YOU MIGHT WANT TO FORMALLY DEFINE DOES NOT EXIST AS IS NOT LEBESGUE INTEGRABLE!
The signal $x(t) = 1$ cannot be input to the integrator because the integral $\int_{-\infty}^{t} dt$ is not finite for any $t$.  However, the integrator $I_\infty$ can operate on absolutely integrable signals because, if $x$ is absolutely integrable, then 
\[
I_\infty x(t) = \int_{-\infty}^{t} x(\tau) d\tau \leq \int_{-\infty}^{t} \abs{x(\tau)} d\tau <  \int_{-\infty}^{\infty} \abs{x(\tau)} d\tau = \|x\|_1 < \infty
\] 
for all $t \in \reals$.  We might then choose a domain for $I_\infty$ as the set of absolutely integrable signals $L^1$.  The integrator can also be applied to signals that are right sided and locally integrable because, for any right sided signal $x$ there exists $T \in \reals$ such that $x(t) = 0$ for all $t < T$ and so,
\[
I_\infty x(t) = \int_{-\infty}^{t} x(\tau) d\tau = \int_{T}^{t} x(\tau) d\tau < \infty
\]
for all $t \in \reals$ if $x$ is locally integrable.  So another possible domain for $I_\infty$ is the set of right sided locally integrable signals.  A final possible domain is the subset of locally integrable signals for which $\int_{-\infty}^0 \abs{x(t)} dt$ is finite.  This last example will be the domain we usually choose for the integrator $I_\infty$.

%The domain used for a given system will usually be obvious from the context in which the system is defined.  For this reason we will not usually state the domain explicitly.  We will only do so if there is chance for confusion.  %When describing the properties of a system we will often want to show that it satisfies certain conditions ``for all signals''.  By this phrase we will always implicitly mean ``for all signals to which the system can be applied''.  %We say this once and for all so that the text is not littered with technical requirements regarding the sets of signals for which particular properties will hold for particular systems.

%%some styles for block diagrams
\begin{figure}[p]
\begin{shaded}
\centering
  \begin{tikzpicture}
    \selectcolormodel{gray} 
    \begin{axis}[compat=newest,font=\footnotesize,height=8cm,width=12cm,xlabel={time (s)},ylabel={electrical potential}, legend style={draw=none,fill=none,legend pos=north east,cells={anchor=west},font=\footnotesize},xmin=999,xmax=1021,ytick={0}, yticklabels={0},xtick={1000,1005,1010,1015,1020},xticklabels={1.000,1.005,1.010,1.015,1.020}]
      \addplot[mark=none] table[x index=0, y index=1] {tests/voltagedivider/voltagedivider.csv};
      \addplot[mark=o,mark repeat=15,mark options={solid,fill=black,scale=1.1}] table[x index=0, y index=2] {tests/voltagedivider/voltagedivider.csv};
      \addplot[mark=*,mark repeat=15,mark options={solid,fill=black,scale=0.6}] table[x index=0, y index=3] {tests/voltagedivider/voltagedivider.csv};
      \legend{$\tilde{x}$, $\tilde{y}$, $\tfrac{42}{57}\tilde{x}$ }
   \end{axis} 
  \end{tikzpicture}  
\captionsetup{type=figure}
%\includegraphics{tests/voltagedivider/plot-1.mps}
\captionof{figure}{Plot of reconstructed input signal $\tilde{x}$ (solid line), output signal $\tilde{y}$ (solid line with circle) and hypothesised output signal $\tfrac{47}{57}\tilde{x}$ (solid line with dot) for the voltage divider circuit in Figure~\ref{circ:voltagedivider}. The hypothesised signal does not match $\tilde{y}$.  One reason is that the model does not take account of the circuitry inside the soundcard.} \label{fig:test1voltagedivider}
%\end{center}
\end{shaded}
\end{figure}


\begin{figure}[p]
  \centering
  \begin{tikzpicture}[domain=-4:6,samples=200]
    % \draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
    \draw[->] (-4.5,0) -- (6.5,0) node[above] {$t$};
    \draw[->] (0,-0.3) -- (0,2.5);
    \draw[smooth,color=black,thick] plot function{2*exp(-(x-2)*(x-2))};
    \draw[smooth,color=black,thick] plot function{2*exp(-(2*x-2)*(2*x-2))};
    \draw[smooth,color=black,thick] plot function{2*exp(-(0.5*x-2)*(0.5*x-2))};
    \draw[smooth,color=black,thick] plot function{2*exp(-(-x-2)*(-x-2))};
    \node[shift={(2,2.2)}] {$\alpha=1$};
    \vtick{2} node[pos=0.5,below] {$2$};
    \node[shift={(-2,2.2)}] {$\alpha=-1$};
    \vtick{-2} node[pos=0.5,below] {$-2$};
    \node[shift={(1,2.2)}] {$\alpha=2$};
    \vtick{1} node[pos=0.5,below] {$1$};
    \node[shift={(4,2.2)}] {$\alpha=1/2$};
    \vtick{4} node[pos=0.5,below] {$4$};
  \end{tikzpicture}
  \caption{Time-scaler system $Hx(t) = x(\alpha t)$ for $\alpha=-1,\tfrac{1}{2}, 1$ and $2$ acting on the signal $x(t) = e^{-(t-2)^2}$.} \label{fig:timescaler}
\end{figure}



\section{Some important systems}\label{sec:some-import-syst}

The system
\[
T_\tau x(t) = x(t - \tau)
\]
is called a \term{time-shifter} or simply \term{shifter}.  This system shifts the input signal along the $t$ axis (``time'' axis) by $\tau$.  When $\tau$ is positive $T_{\tau}$ delays the input signal by $\tau$.  The shifter will appear so regularly that we use the special notation $T_\tau$ to represent it.  Figure~\ref{fig:timeshifter} depicts the action of shifters $T_{1.5}$ and $T_{-3}$ on the signal $x(t) = e^{-t^2}$.  When $\tau=0$ the shifter is the \term{identity system} $T_0 x = x$ that maps a signal to itself.  Another important system is the \term{time-scaler} that has the form
\[
H x(t) = x(\alpha t), \qquad \alpha \in \reals.
\]
Figure~\ref{fig:timescaler} depicts the action of time-scalers with different values for $\alpha$.  When $\alpha=-1$ the time-scaler reflects the input signal in the $t$ axis.  When $\alpha = 1$ the time-scaler is the identity system $T_0$.  Both the shifter and time-scaler are well defined for all signals and so it is reasonable to choose their domains to be the entire set of signals $\reals \to \complex$.  We always assume this is the case unless otherwise stated.

% Another important system is the \term{scaler} that has the form
% \[
% S_\alpha(x,t) = x(t/\alpha)
% \]
% where $\alpha$ is a real number not equal to zero.  Figure~\ref{fig:timescaler} depicts the action of a scaler with $\alpha=-1,\tfrac{1}{2}, 1$.  When $\alpha=-1$ the scaler reflects the input signal in the time axis and in that case we will often denote it by the letter $R$, that is, we put $R = S_{-1}$.  We call $R$ the \term{time reflection system}.  When $\alpha=1$ the system $S_1 = T_0$ is the identity system.  The identity system is also obtained by applying the time reflection system twice, that is, $R(R(x)) = R^2(x) = x$.

% \begin{figure}[tp]
%   \centering
%   \begin{tikzpicture}[domain=-4:6,samples=200]
%     % \draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);
%     \draw[->] (-4.5,0) -- (6.5,0) node[above] {$t$};
%     \draw[->] (0,-0.3) -- (0,2.5);
%     \draw[smooth,color=black,thick] plot function{2*exp(-(x-2)*(x-2))};
%     \draw[smooth,color=black,thick] plot function{2*exp(-(2*x-2)*(2*x-2))};
%     \draw[smooth,color=black,thick] plot function{2*exp(-(0.5*x-2)*(0.5*x-2))};
%     \draw[smooth,color=black,thick] plot function{2*exp(-(-x-2)*(-x-2))};
%     \node[shift={(2,2.2)}] {$x$};
%     \vtick{2} node[pos=0.5,below] {$2$};
%     \node[shift={(-2,2.2)}] {$R(x)$};
%     \vtick{-2} node[pos=0.5,below] {$-2$};
%     \node[shift={(1,2.2)}] {$S_{1/2}(x)$};
%     \vtick{1} node[pos=0.5,below] {$1$};
%     \node[shift={(4,2.2)}] {$S_2(x)$};
%     \vtick{4} node[pos=0.5,below] {$4$};
%   \end{tikzpicture}
%   \caption{scaler system $S_\alpha(x,t) = x(t/\alpha)$ for $\alpha=-1,\tfrac{1}{2}, 1$ and $2$ acting on the signal $x(t) = e^{-(t-2)^2}$.  When $\alpha=-1$ we use the notation $R(x) = S_{-1}(x)$.} \label{fig:timescaler}
% \end{figure}

% A simple by regularly encountered signal is the \term{amplifier}
% \[
% H(x,t) = a x(t)
% \]
% where $a \in \reals$.  
Another regularly encountered system is the \term{differentiator}
\[
Dx(t) = \frac{d}{dt} x(t)
\]
that returns the derivative of the input signal.  We also define a $k$th differentiator
\[
D^k x(t) = \frac{d^k}{dt^k} x(t)
\]
that returns the $k$th derivative of the input signal.  %BLERG: existance, special definition at discontinuities?
The differentiator is only defined for differentiable signals.  A domain for $D$ is the set of differentiable signals $C^1$ and a domain for $D^k$ is the set of $k$-times differentiable signals $C^k$.  Unless otherwise stated we will always assume the domain of $D^k$ to be $C^k$.

% We must be careful when choosing a domain for the differentiator.  For example, consider the step function $u(t)$ from~\eqref{eq:stepfunction}.  We cannot differentiate $u(t)$ at $t = 0$ because
% \[
% D u(0) = \lim_{h \to 0} \frac{u(0) - u(-h)}{h} = \lim_{h \to 0} \frac{1}{h} 
% \]
% and the limit does not converge.

Another important system is the \term{integrator}
\[
I_a x(t) = \int_{-a}^{t} x(\tau) d\tau.
\]
The parameter $a$ describes the lower bound of the integral.  In this course it will often be that $a=\infty$.  For example, the response of the integrator $I_{\infty}$ to the signal $t u(t)$ where $u$ is the step function~\eqref{eq:stepfunction} is
\[
\int_{-\infty}^{t} \tau u(\tau) d\tau = \begin{cases}
\int_{0}^{t} \tau d\tau = \frac{t^2}{2} & t > 0 \\
0 & t \leq 0.
\end{cases}
\]
Observe that the integrator $I_\infty$ cannot be applied to the signal $x(t) = t$ because $\int_{-\infty}^{t} \tau d\tau$ is not finite for any $t$.  A domain for $I_\infty$ cannot contain the signal $x(t) = t$.  %Example of domains for $I_\infty$ are the set of absolutely integrable signals $L^1$ and the set of signals that are both right sided and locally integrable.  
Unless otherwise stated we will assume the domain of $I_\infty$ to be the subset of locally integrable signals $L_{\text{loc}}$ for which $\int_{-\infty}^0 \abs{x(t)} dt < \infty$ (Exercise~\ref{excer:domainintegrator}).  For finite $a < \infty$ we will assume, unless otherwise stated, that the domain of $I_a$ is the set of locally integrable signals $L_{\text{loc}}$. %The set of all linear combinations of signals of $L^1$ and right sided locally integrable signals also forms a domain.


\section{Properties of systems}\label{sec:properties-systems}

In this section we define a number of important properties that systems can possess.  In what follows $X \subseteq \reals \to \complex$ and $Y \subseteq \reals \to \complex$ are set of signals.  A system $H \in X \to Y$ with domain $X$ is called \term{memoryless} if, for all input signals $x \in X$, the output signal $Hx$ at time $t$ depends only on $x$ at time $t$.  For example $\frac{1}{x(t)}$ with domain the set of signals that do not take the value zero and the identity system $T_0$ with domain the set of all signal $\reals \to \complex$ are memoryless, but 
\[
x(t) + 3 x(t-1) \qquad \text{and} \qquad \int_{0}^{1} x(t - \tau) d\tau \qquad x \in L_{\text{loc}}
\] 
are not.  A shifter $T_\tau$ with $\tau \neq 0$ is not memoryless.

A system $H \in X \to Y$ is \term{causal} if, for all input signals $x \in X$, the output signal $Hx$ at time $t$ depends on $x$ at times less than or equal to $t$.  Memoryless systems such as $\frac{1}{x(t)}$ and $T_0$ are also causal.  The shifter $T_\tau$ is causal when $\tau \geq 0$, but is not causal when $\tau < 0$.  The systems 
\[
x(t) + 3 x(t-1) \qquad \text{and} \qquad \int_{0}^{1} x(t - \tau) d\tau \qquad x \in L_{\text{loc}}
\] 
are causal, but the systems 
\[
x(t) + 3 x(t+1) \qquad \text{and} \qquad \int_{0}^{1} x(t + \tau) d\tau \qquad x \in L_{\text{loc}}
\] 
are not causal.

A system $H \in X \to Y$ is called \term{bounded-input-bounded-output (BIBO) stable} or just \term{stable} if the output signal $Hx$ is bounded whenever the input signal $x$ is bounded.  That is, $H$ is stable if for every positive real number $M$ there exists a positive real number $K$ such that for all input signals $x \in X$ bounded below $M$, that is,
\[
\abs{x(t)} < M \qquad \text{for all $t\in\reals$},
\] 
it holds that the output signal $Hx$ is bounded below $K$, that is,
\[
\abs{Hx(t)} < K \qquad \text{for all $t\in\reals$}.
\]
For example, the system $x(t) + 3 x(t-1)$ with domain $\reals \to \complex$ is stable with $K = 4M$ since if $\abs{x(t)} < M$, then 
\[
\abs{x(t) + 3 x(t-1)} \leq \abs{x(t)} + 3 \abs{x(t-1)} < 4 M = K.
\]
The integrator $I_a$ for $a \in \reals$ having domain $L_{\text{loc}}$ and the differentiator $D$ with domain $C^1$ are not stable (Exercises~\ref{excer:integratornotstable}~and~\ref{excer:diffnotstable}).

Let $H \in X \to Y$ be a system with both the domain $X$ and $Y$ being linear spaces of signals.  The system $H$ is \term{linear} if
\[
H( ax + by) = a Hx + b Hy
\]
for all signals $x, y \in X$ and all complex numbers $a$ and $b$.  That is, a linear system has the property: if the input consists of a weighted sum of signals, then the output consists of the same weighted sum of the responses of the system to those signals.  Figure~\ref{blockdiag:linearsystem} indicates the linearity property using a block diagram.  For example, the differentiator is linear because
\begin{align*}
D(ax + by)(t) &= \frac{d}{dt}\big(ax(t) + by(t)\big) 
&= a\frac{d}{dt}x(t) + b\frac{d}{dt}y(t) 
&= aDx(t) + bDy(t)
\end{align*}
whenever both $x$ and $y$ are differentiable, that is, when $x,y \in C^1$.  
%BLERG, there are significant caveats here, what happens when one of x or y is not differentiable, but the sum is!!!!
However, the system $Hx(t) = \frac{1}{x(t)}$ is not linear because
\[
H(ax + by)(t) = \frac{1}{ax(t) + by(t)} \neq \frac{a}{x(t)} + \frac{b}{y(t)} = aHx(t) + bHy(t)
\]
in general.  %Throughout this text the prhase ``Let $H \in X \to Y$ be a linear shift-invariant system'' will implicitly 

% The property of linearity trivially generalises to more than two signals.  For example, if $x_1,\dots,x_k$ are signals and $a_1,\dots,a_k$ are complex numbers for some finite $k$, then
% \[
% H( a_1x_1 + \dots + a_k x_k ) = a_1 H x_1 + \dots + a_k H x_k.
%\]
% With a little care, the prpoperty of linearity can be extended to infinite sums, that is
% \[
% H\left( \sum_{n=1}^\infty a_n x_n \right) = \sum_{n=1}^\infty H(a_n x_n)
% \]
% under certain conditions on the system $H$ and the infinite sequence of signals $\{x_n\}$ and sequence complex numbers $\{a_n\}$.  Precisely formalising the conditions underwhich this property holds is beyond our scope.  The property of linearity even extends to integrals, that is, if $\{y_\kappa, \kappa \in \reals\}$ is a set of signals parameterised by $\kappa$ then
% \begin{equation}\label{eq:exchangesystemandintegration}
% H \left( \int_a^b y_\kappa d\kappa \right) = \int_a^b H \left( y_\kappa \right) d\kappa,
% \end{equation}
% under certain conditions on $H$ and $\{y_\kappa\}$.  This property can hold even when the bounds of the integral are infinite, i.e, $a=-\infty$ or $b=\infty$.  We will have use of this property in Section~\ref{sec:laplace-transform} when we discuss the Laplace transform.  We will not attempt to formally state the conditions required for~\eqref{eq:exchangesystemandintegration} to hold in this course.

% \[
% z = \lim_{m\rightarrow\infty} \sum_{k=0}^m a_k x_k = \sum_{k=0}^\infty a_kx_k
% \] 
% exists (i.e. the limit exists) and can be applied to $H$.  Then
% \[
% H(z) = \lim_{m\rightarrow\infty} \sum_{i=1}^m H( a_i x_i )= \sum_{i=1}^{\infty} H( a_i x_i ).
% \]
% if the limit exists.
% We a little care, the property of linearity can be extended to integrals rather than just sums.  Let $H$ be a linear system, and let $\{ x_\tau, \tau \in \reals \}$ be a set of signals parameterised by $\tau \in \reals$, each $x_{\tau}$ able to be applied to $H$.  For given real number $a,b$ with $a < b$, assume that the signal $z = \int_a^b x_\tau d\tau$ exists (i.e. the integral exists) and that the system $H$ can be applied to $z$.  Then
% \begin{equation}\label{eq:Hintexchange}
% \int_{a}^{b} H(x_\tau, t ) d\tau = H( z , t ) = H( \int_a^b x_\tau d\tau  , t ).
% \end{equation}
% That is, when the appropriate integral exist, the order of integration and $H$ can be exchanged.  This result can hold even when the bound of the integral are infinite, i.e. $a=-\infty$ and $b = \infty$.  BLERG: Exchange of infinite sums? This is handwavy, but it might need to be!



\begin{figure}[tbp]
\centering
\begin{tikzpicture}[node distance=1.8cm,auto,>=latex']
    \node[dspadder] (adder) {};
    \node[dspsquare] (H) [right of=adder, node distance=1.5cm] {$H$};
    \node[dspmixer,dsp/label=right] (xmult) [above of=adder, node distance=1cm] {$a$};
    \node [dspnodeopen,dsp/label=above] (x) [left of=xmult, node distance=1cm] {$x$};
    \node[dspmixer,dsp/label=right] (ymult) [below of=adder, node distance=1cm] {$b$};
    \node [dspnodeopen] (y) [left of=ymult, node distance=1cm,label=above:$y$] {};
    \node [dspnodeopen] (res) [right of=H,label=above:$H(ax + by)$] {};
    \draw[dspconn] (x) -- (xmult);
    \draw[dspconn] (y) -- (ymult);
    \draw[dspconn] (ymult) -- (adder);
    \draw[dspconn] (xmult) -- (adder);
    \draw[dspconn] (adder) -- (H);
    \draw[dspflow] (H) -- (res);
%\end{tikzpicture}
%\qquad
%\begin{tikzpicture}[node distance=1.8cm,auto,>=latex']
    \node[dspadder] (adder1) [right of=adder, node distance=7.25cm] {};
    \node[dspmixer,dsp/label=right] (xmult1) [above of=adder1, node distance=1cm] {$a$};
    \node[dspsquare] (xH1) [left of=xmult1, node distance=1cm] {$H$};
    \node [dspnodeopen,dsp/label=above] (x1) [left of=xH1, node distance=1cm] {$x$};
    \node[dspmixer,dsp/label=right] (ymult1) [below of=adder1, node distance=1cm] {$b$};
    \node[dspsquare] (yH1) [left of=ymult1, node distance=1cm] {$H$};
    \node [dspnodeopen] (y1) [left of=yH1, node distance=1cm,label=above:$y$] {};
    \node [dspnodeopen] (res1) [right of=adder1,label=above:$aHx+ bHy$] {};
    \draw[dspconn] (x1) -- (xH1);
    \draw[dspconn] (xH1) -- (xmult1);
    \draw[dspconn] (xmult1) -- (adder1);
    \draw[dspconn] (y1) -- (yH1);
    \draw[dspconn] (yH1) -- (ymult1);
    \draw[dspconn] (ymult1) -- (adder1);
    \draw[dspflow] (adder1) -- (res1);
\end{tikzpicture}
\caption{If $H$ is a linear system the outputs of these two diagrams are the same signal, i.e. $H(ax+by) = aHx+bHy$.}\label{blockdiag:linearsystem}
\end{figure}

Let $H \in X \to Y$ be a system with both the domain $X$ and $Y$ being shift-invariant spaces. The system $H$ is \term{shift-invariant} (or \term{time-invariant}) if
\[
H T_\tau x(t) = Hx(t-\tau)
\]
for all signals $x \in X$ and all shifts $\tau\in\reals$.  That is, a system is shift-invariant if shifting the input signal results in the same shift of the output signal.  Equivalently, $H$ is shift-invariant if it commutes with the shifter $T_\tau$, that is, if
\[
H T_\tau x = T_\tau Hx
\]
for all $\tau \in \reals$ and all signals $x \in X$.  Figure~\ref{blockdiag:timeinvariance} represents the property of shift-invariance with a block diagram.  For example, the differentiator is shift-invariant because
\[
DT_\tau x(t) = \frac{d}{dt}x(t - \tau) = T_\tau Dx(t)
\]
by the chain rule for differentiation.  The integrator $I_\infty$ and shift-invariant (Exercise~\ref{exer:Iinftylineshiftinv}) but the integrator $I_a$ for finite $a < \infty$ is not (Exercise~\ref{exer:Ialineshiftinv}).
% A consequence of time-invariance is
% \begin{equation}\label{eq:timeinvvarchange}
% H(x, t+\tau) = H\big(T_{-\tau}(x), t\big) = H\big(T_{-t}(x), \tau\big).
% \end{equation}
% This property will be useful in later sections. 

We will be primarily interested in systems that are both linear and shift-invariant.  Such systems are said to be \term{linear shift-invariant} or \term{linear time-invariant} systems.  The phrase ``Let $H \in X \to Y$ be a linear shift-invariant system'' will occur regularly and it will always imply that both sets $X$ and $Y$ are linear and shift-invariant spaces of signals.

\begin{figure}[tbp]
\centering
\begin{tikzpicture}[node distance=1.3cm,auto,>=latex']
    \node[dspnodeopen,dsp/label=above] (x) {$x$};
    \node[dspsquare] (H) [right of=x] {$H$};
    \node[dspsquare] (D) [right of=H] {$T_\tau$};  
    \node[dspnodeopen,dsp/label=above] (out) [right of=D,node distance=1.7cm,label=above:$T_\tau Hx$] {};
    \draw[dspconn] (x) -- (H);
    \draw[dspconn] (H) -- (D);
    \draw[dspflow] (D) -- (out);
\end{tikzpicture}
\qquad
\begin{tikzpicture}[node distance=1.3cm,auto,>=latex']
  \node[dspnodeopen,dsp/label=above] (x) {$x$};
  \node[dspsquare] (D) [right of=x] {$T_\tau$};  
  \node[dspsquare] (H) [right of=D] {$H$};
  \node[dspnodeopen,dsp/label=above] (out) [right of=H,node distance=1.7cm,label=above:$HT_\tau x$] {};
  \draw[dspconn] (x) -- (D);
  \draw[dspconn] (D) -- (H);
  \draw[dspflow] (H) -- (out);
\end{tikzpicture}
\caption{If $H$ is a shift-invariant system the outputs of these two diagrams are the same signal, i.e. $HT_\tau x = T_\tau Hx$.}\label{blockdiag:timeinvariance}
\end{figure}

% Let $S$ be a set of signals.  A system $H$ is said to be \term{invertible} on $S$ if each signal $x \in S$ is mapped to a unique signal $H(x)$.  That is, for all signals $x,y \in S$ then $H(x) = H(y)$ if and only if $x = y$.  If a system $H$ is invertible on $S$ then there exists an inverse system $H^{-1}$ such that
% \[
% x = H^{-1}\big(H(x)\big) \qquad \text{for all $x \in S$}.
% \]
% % For example, let $S$ be the set of all continuous-time signals\footnote{In this course the set of all continuous-time signals is the set of all locally integrable functions from $\reals \to \complex$.} and let $H$ be the system
% % \[
% % H(x,t) = \int_{-1}^0 x(t) dt - \int_0^1x(t) dt.
% % \]
% % Let $x$ be a continuous-time signal and let $y(t) = x(t) + c$ for a constant $c \neq 0$.  Then
% % \begin{align*}
% % H(x,t) &= \int_{-1}^0 x(t) dt - \int_0^1x(t) dt \\
% % &= \int_{-1}^0 x(t) dt - \int_0^1x(t) dt + \int_{-1}^1 (c - c) dt \\
% % &= \int_{-1}^0(x(t)+c) dt - \int_0^1(x(t)+c) dt \\
% % &= \int_{-1}^0 y(t) dt - \int_0^1y(t) dt = H(y,t),
% % \end{align*}
% For example, let $S$ be a any set of signals.  The shifter $T_\tau$ is invertible on $S$.  The inverse system is $T_{-\tau}$ since
% \[
% T_{-\tau}\big(T_\tau(x), t\big) = x(t - \tau + \tau) = x(t).
% \]
% As another example, let $S$ be the set of differentiable signals.  The differentiator system $D$ is \term{not} invertible on $S$ because if $x \in S$ and if $y(t) = x(t) + c$ for any constant $c$ then $D(y) = D(x)$.  However, if we restrict $S$ to those differentiable signals for which $x(0) = c$ is fixed, then $D$ \emph{is} invertible on $S$.  The inverse system in this case is
% \[
% D^{-1}(x,t) = I_0(x,t) + c = \int_0^t x(t) dt + c
% \]
% because
% \[
% D^{-1}(D(x),t) = \int_0^t D(x,t) dt + c = \int_0^t \frac{d}{dt} x(t) dt + x(0) = x(t)
% \]
% by the fundamental theorem of calculus.



\begin{advanced}

\section{Convergence of signals}

A signal $x$ is said to have a limit $\ell$ at $p$ from below if for any $\epsilon > 0$ there exists a $t_0 <  p$ such that $\abs{x(t) - \ell} < \epsilon$ for all $t$ in the open interval $(t_0, p)$.  Similarly $x$ is said to have a limit $\ell$ from above at $p$ if for any $\epsilon > 0$ there exists a $t_0 > p$ such that $\abs{x(t) - \ell}$ for all $t$ in the open interval $(p, t_0)$.  As is customary, we write
\[
\lim_{t \to p^-} x(t) = \ell, \qquad \lim_{t \to p^+} x(t) = \ell
\]
to indicate limits from below and above respectively.  In the case where $p = \pm \infty$ the definition of a limit is the same, for example, $\lim_{t \rightarrow \infty} x(t) = \ell$ means that for any $\epsilon > 0$ there is a $t_0$ such that $\abs{x(t) - \ell} < \epsilon$ for all $t > t_0$.

A signal $x$ is \term{continuous} at $p$ if the limits of $x$ as $t \rightarrow p$ from above and below are equal to $x(p)$, i.e.,
\[
\lim_{t \to p^-} x(t) = \lim_{t \to p^+} x(t) = x(p).
\]
In other words, $x$ is continuous at $p$ if for any $\epsilon > 0$ there exists a $\delta$ such that $\abs{x(t) - \ell} < \epsilon$ whenever $\abs{t - p} < \delta$.  Observe that $\delta$ in the previous definition may depend on $p$.  A signal is said to be uniformly continuous if $\delta$ can be chosen independently of $p$.  That is, a signal $x$ is \term{uniformly continuous} if for any $\epsilon > 0$ there exists a $\delta$ such that $\abs{t - p} < \delta$ implies that $\abs{x(t) - x(p)} < \epsilon$ for all $p \in \reals$.  Written another way, a signal $x$ is uniformly continuous if
\[
\abs{t - p} < \delta \Rightarrow \sup_{p \in \reals} \abs{x(t) - x(p)} < \epsilon,
\]
where $\sup$ is the supremum.  If $A$ is a set of real numbers then $\sup_{x \in A} x$ is the smallest real number greater than or equal to all of the elements in $A$. 

BLERG: Pointwise/uniform convergence of functions, convergence in norm.


\section{Continuity of systems}

BLERG: See Theorem~5.4 of~\cite{Rudin_real_and_complex_analysis}.  For linear systems boundedness (or stability with respect to specific norms) is the some as continuity in those norms.  This will be the key to making sense of the Laplace and Fourier transforms (etc) of output signals.

\end{advanced}


% Start your notes here.
%BLERG: TO DO
% Potentially add to Section 1, definition of limit, continuity, absolute continuity, convergence of signal (pointwise and uniform), and continuity of systems (pointwise, uniform etc).
% We assume/hypothesise that practical systems (electrical, mechanical etc)  are linear and time invariant.  This does not follow from the differential equations.  It's a hypothesis that should be tested like any other.  It's also likely necessary to assume continuity and single valuedness,   See Zemanian Chapter 10 - Passive Systems.  It might be worth writting something on this.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main.tex"
%%% End: 
